---
title: "Getting_Started_With_Rstudio"
author: "Tye Leckinger"
date: '2023-01-24'
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(tm)
library(keras)
```

```{r Functions}
ungroup_by <- function(.data, by) {
  data <- .data#create temp
  data <- data[-(1:nrow(data)),]#remove rows
  data <- data[-by]#remove n col
  
  for (i in 1:nrow(.data)) {#For 1 to the length of row in data
    n <- as.double(.data[i,by])#get the n in the i'th iteration
    if(n != 0) {#If n does not equal to 0
      for (k in 1:n) { #for n amout of iterations
        data[nrow(data)+1,] <- .data[i,-by] #add the current row i into the temp table
      }
    }
  
  }
  return(data) #return the temp table
  
  
  
}
frequency_matrix <- function(source) {
  require("tm")
  text_Corpus <- source %>% VectorSource() %>% VCorpus()
  clean_corpus <- function(corpus) {
    corpus %>% 
    tm_map(stripWhitespace) %>%
    tm_map(removePunctuation) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removeWords, stopwords("en")) %>%
    tm_map(stemDocument)
  }
  text_Corpus <- text_Corpus %>% clean_corpus()
  text_matrix <- DocumentTermMatrix(text_Corpus) %>% as.matrix()
  text_matrix
}
generate_sequences <- function(train_data,#training data
                               validation_data,# validation data
                               maxlen = 50,#maximum length of the embedding sequence
                               max_words = 2000)#will only choose consider max_words amount of words for the embedding
{
  
  training_text <- train_data$tweets %>% as.array()#get the text
  validation_text <- validation_data$tweets %>% as.array()#get the text
  
  tokenizer <- text_tokenizer(num_words = max_words) %>%#create and fit tokenizer
  fit_text_tokenizer(training_text)
  
  
  sequences <- texts_to_sequences(tokenizer,training_text) #Translates text to sequences of integers(use the tokenizer$word_index to know which int maps to what word)
  training_sequences <- pad_sequences(sequences, maxlen = maxlen)#make all sequences the same length with the length being maxlen
  sequences <- texts_to_sequences(tokenizer,validation_text) #Translates text to sequences of integers(use the tokenizer$word_index to know which int maps to what word)
  validation_sequences <- pad_sequences(sequences, maxlen = maxlen)#make all sequences the same length with the length being maxlen
  
  list(train = training_sequences,
       validation = validation_sequences,
       tokenizer = tokenizer
       )
}


print_percent <- function(i, length, p) {
  percent <- floor((i/length * 100))
  if(percent %% 10 == 0 && p != percent){
      print(paste0(percent,"% Complete"))
      p = percent
    }
    return(p)
  
  
}

get_hashtags_df <- function(text) {
  
  tweets <- text
  
  tweets_separated <- tweets %>% str_split(pattern = " ")
  y <- list()
  p = 0
  for (i in 1:length(tweets_separated)) {
    hashtags <- list()
    
    for(k in 1:length(tweets_separated[[i]])){
      if(grepl(tweets_separated[[i]][k], pattern = "#.*")){
        hashtags <- append(hashtags,tweets_separated[[i]][k])
      }
    }
    y <- append(y,list(hashtags))
    
    
    p = print_percent(i,length = length(tweets_separated), p = p)
    #print()#," Percent complete")
    
    #tweets_separated[[i]]
    
  }
  return(y)
}
```

```{r}
Dir <- 'C:/Users/tyele/Documents/Datasets/Tweets_with_Sarcasm_and_Irony'
train <- read_csv(paste0(Dir,"/train.csv"))
test <- read_csv(paste0(Dir,"/test.csv"))
test <- test %>% filter(!is.na(class))


```
```{r}
classes <- train$class %>% unique()
num_obs_train <- nrow(train)
num_obs_test <- nrow(test)
```

Classes:  `r classes`

Number of observations in Train: `r num_obs_train`

Number of observations in Test: `r num_obs_test`
```{r}




max_words = 1000
samples = 100000
embedding_dim = 8
maxlen = 50



sequences_dat <- generate_sequences(train,
                                    test,
                                    max_words = max_words,
                                    maxlen = maxlen)

training_sequences <- sequences_dat$train
test_sequences <- sequences_dat$validation


model <-  keras_model_sequential() %>%
  layer_embedding(input_dim = max_words,
                  output_dim = embedding_dim,
                  input_length = maxlen) %>%
  layer_flatten() %>%
  layer_dense(units = 1, 
              activation = "sigmoid") 

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

#tensor_log_dir <- c("Tensor_logs")
#tensorboard(tensor_log_dir)
#callbacks = list(
#  callback_tensorboard(
#    log_dir = tensor_log_dir,
#    histogram_freq = 1,
#    embeddings_freq = 1
#  )
#)

history <- model %>% fit(
  training_sequences,
  training_labels,
  epochs = 15,
  batch_size = 512#,
  #validation_data = list(validation_sequences,validation_labels)#,
 # callbacks = callbacks
)

```

```{r}
Dir <- 'C:/Users/tyele/Documents/Datasets/Tweets_with_Sarcasm_and_Irony'
train <- read_csv(paste0(Dir,"/train.csv"))
#tweets <- train$tweets
y <- get_hashtags_df(train$tweets)
train$tweets <- train$tweets %>% sub(pattern = "#.* | #.*$", replacement = "")
train <- train %>% mutate(hashtags = y)
  
test <- read_csv(paste0(Dir,"/test.csv"))
  

```



